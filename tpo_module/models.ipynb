{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "# import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications import ResNet50, ResNet50V2\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Input, concatenate, Flatten, \n",
    "                                     Activation, Dropout, Bidirectional)\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as preprocess_inputV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, optimizer, loss, metrics):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(dim1, dim2, optimizer, loss, metrics):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128, input_shape=(dim1,dim2), recurrent_activation='relu')))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create resnet50 model\n",
    "def create_resnet50(height, width, depth, optimizer, loss, metrics):\n",
    "    # Input layer\n",
    "    input_layer=Input(shape=(height,width,depth))\n",
    "    # set trainable layers in ResNet50\n",
    "    resnet=ResNet50(include_top=False, input_shape=(height,width,depth), weights='imagenet', pooling='avg')\n",
    "    output=resnet.layers[-1].output\n",
    "    output=Flatten()(output)\n",
    "    resnet=Model(resnet.input, output)\n",
    "    resnet.trainable=True\n",
    "    for layer in resnet.layers[:144]:\n",
    "        layer.trainable=False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "   \n",
    "    model = resnet(input_layer)\n",
    "    model = Dense(1, activation='sigmoid')(model)\n",
    "    model = Model(input_layer, model)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create resnet50v2 model\n",
    "def create_resnet50v2(height, width, depth, optimizer, loss, metrics):\n",
    "    # Input layer\n",
    "    input_layer=Input(shape=(height,width,depth))\n",
    "    # set trainbale layers in ResNet50v2\n",
    "    resnet = ResNet50V2(include_top=False, input_shape=(height,width,depth), weights='imagenet', pooling='avg')\n",
    "    output = resnet.layers[-1].output\n",
    "    output = Flatten()(output)\n",
    "    resnet = Model(resnet.input, output)\n",
    "    resnet.trainable = True\n",
    "    for layer in resnet.layers[:144]:\n",
    "        layer.trainable=False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "    \n",
    "    model = resnet(input_layer)\n",
    "    model = Dense(1, activation='sigmoid')(model)\n",
    "    model = Model(input_layer, model)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mlp&resnet50 model\n",
    "def create_mlpresnet50(dim, height, width, depth, optimizer, loss, metrics):\n",
    "    #A model\n",
    "    modelA = Sequential()\n",
    "    modelA.add(Dense(128, input_dim=dim, activation=\"relu\"))\n",
    "    modelA.add(Dropout(0.2))\n",
    "    modelA.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # B model\n",
    "    # Input layer\n",
    "    input_layer=Input(shape=(height,width,depth))\n",
    "    # set trainable layers in ResNet50\n",
    "    resnet=ResNet50(include_top=False, input_shape=(height,width,depth), weights='imagenet', pooling='avg')\n",
    "    output=resnet.layers[-1].output\n",
    "    output=Flatten()(output)\n",
    "    resnet=Model(resnet.input, output)\n",
    "    resnet.trainable=True\n",
    "    for layer in resnet.layers[:144]:\n",
    "        layer.trainable=False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "   \n",
    "    modelB = resnet(input_layer)\n",
    "    modelB = Dense(1, activation='sigmoid')(modelB)\n",
    "    modelB = Model(input_layer, modelB)\n",
    "    \n",
    "    #concatenate two models\n",
    "    combined = concatenate([modelA.output, modelB.output])\n",
    "    model_output = Dense(2, activation='relu')(combined)\n",
    "    model_output = Dense(1, activation='sigmoid')(model_output)\n",
    "    model = Model([modelA.input,modelB.input], model_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lstm&resnet50 model\n",
    "def create_lstmresnet50(batch, dim, height, width, depth, optimizer, loss, metrics):\n",
    "    #A model\n",
    "    modelA = Sequential()\n",
    "    modelA.add(Input(shape=(batch, dim)))\n",
    "    modelA.add(Bidirectional(LSTM(128, recurrent_activation='relu')))\n",
    "    modelA.add(Dropout(0.2))\n",
    "    modelA.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # B model\n",
    "    # ResNet Input layer\n",
    "    input_layer=Input(shape=(height,width,depth))\n",
    "    # set trainable layers in ResNet50\n",
    "    resnet=ResNet50(include_top=False, input_shape=(height,width,depth), weights='imagenet', pooling='avg')\n",
    "    output=resnet.layers[-1].output\n",
    "    output=Flatten()(output)\n",
    "    resnet=Model(resnet.input, output)\n",
    "    resnet.trainable=True\n",
    "    for layer in resnet.layers[:144]:\n",
    "        layer.trainable=False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "   \n",
    "    modelB = resnet(input_layer)\n",
    "    modelB = Dense(1, activation='sigmoid')(modelB)\n",
    "    modelB = Model(input_layer, modelB)\n",
    "    \n",
    "    #concatenate two models\n",
    "    combined = concatenate([modelA.output, modelB.output])\n",
    "    model_output = Dense(2, activation='relu')(combined)\n",
    "    model_output = Dense(1, activation='sigmoid')(model_output)\n",
    "    model = Model([modelA.input, modelB.input], model_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mlp&resnet50v2 model\n",
    "def create_mlpresnet50v2(dim, height, width, depth, optimizer, loss, metrics):\n",
    "    #A model\n",
    "    modelA = Sequential()\n",
    "    modelA.add(Dense(128, input_dim=dim, activation=\"relu\"))\n",
    "    modelA.add(Dropout(0.2))\n",
    "    modelA.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #B model\n",
    "    # Input layer\n",
    "    input_layer=Input(shape=(height,width,depth))\n",
    "    # set trainable layers in ResNet50v2\n",
    "    resnet=ResNet50V2(include_top=False, input_shape=(height,width,depth), weights='imagenet', pooling='avg')\n",
    "    output=resnet.layers[-1].output\n",
    "    output=Flatten()(output)\n",
    "    resnet=Model(resnet.input, output)\n",
    "    resnet.trainable=True\n",
    "    for layer in resnet.layers[:144]:\n",
    "        layer.trainable=False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "   \n",
    "    modelB = resnet(input_layer)\n",
    "    modelB = Dense(1, activation='sigmoid')(modelB)\n",
    "    modelB = Model(input_layer, modelB)\n",
    "    \n",
    "    #concatenate two models\n",
    "    combined = concatenate([modelA.output, modelB.output])\n",
    "    model_output = Dense(2, activation='relu')(combined)\n",
    "    model_output = Dense(1, activation='sigmoid')(model_output)\n",
    "    model = Model([modelA.input, modelB.input], model_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lstm&resnet50v2 model\n",
    "def create_lstmresnet50v2(batch, dim, height, width, depth, optimizer, loss, metrics):\n",
    "    #A model\n",
    "    modelA = Sequential()\n",
    "    modelA.add(Input(shape=(batch, dim)))\n",
    "    modelA.add(Bidirectional(LSTM(128, recurrent_activation='relu')))\n",
    "    modelA.add(Dropout(0.2))\n",
    "    modelA.add(Dense(1, activation=\"sigmoid\"))\n",
    "    #B model\n",
    "    # ResNet Input layer\n",
    "    input_layer=Input(shape=(height,width,depth))\n",
    "    # set trainable layers in ResNet50v2\n",
    "    resnet=ResNet50V2(include_top=False, input_shape=(height,width,depth), weights='imagenet', pooling='avg')\n",
    "    output=resnet.layers[-1].output\n",
    "    output=Flatten()(output)\n",
    "    resnet=Model(resnet.input, output)\n",
    "    resnet.trainable=True\n",
    "    for layer in resnet.layers[:144]:\n",
    "        layer.trainable=False\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in resnet.layers]\n",
    "   \n",
    "    modelB = resnet(input_layer)\n",
    "    modelB = Dense(1, activation='sigmoid')(modelB)\n",
    "    modelB = Model(input_layer, modelB)\n",
    "    \n",
    "    #concatenate two models\n",
    "    combined = concatenate([modelA.output, modelB.output])\n",
    "    model_output = Dense(4, activation='relu')(combined)\n",
    "    model_output = Dense(1, activation='sigmoid')(model_output)\n",
    "    model = Model([modelA.input, modelB.input], model_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train model with cross validation fold\n",
    "# def train_model_with_cv(csv_data, image_data, model, cross_validation, epochs, batch_size):\n",
    "#     print('Train Model')\n",
    "#     print(\"==================================================\")\n",
    "#     # split data by 9:1\n",
    "#     print(\"Divided data into a training set and a test set in a ratio of 9:1 randomly\")\n",
    "#     split = train_test_split(csv_data, image_data, test_size=0.1, random_state=42)\n",
    "#     (trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "#     testY = testAttrX[\"TPO\"]\n",
    "#     testAttrX=testAttrX.drop(\"TPO\", axis=1).drop(\"ID\", axis=1)\n",
    "#     print(\"==================================================\")\n",
    "#     # cross validation data\n",
    "#     print('Cross Validation')\n",
    "#     kfold = StratifiedKFold(n_splits = cross_validation, shuffle = False)\n",
    "#     cvscores = []\n",
    "#     iteration = 1\n",
    "#     t = trainAttrX.TPO\n",
    "#     for train_index, valid_index in kfold.split(np.zeros(len(t)),t):\n",
    "#         print('==================================================')\n",
    "#         print(\"Iteration=\", iteration)\n",
    "#         iteration+=1\n",
    "        \n",
    "#         # prepare the data\n",
    "#         trainY=trainAttrX[\"TPO\"].reindex(index=train_index)\n",
    "#         validY=trainAttrX[\"TPO\"].reindex(index=valid_index)\n",
    "#         trainAX=trainAttrX.reindex(index=train_index).drop(\"TPO\", axis=1).drop(\"ID\", axis=1)\n",
    "#         trainBX=trainImagesX[train_index]\n",
    "#         validAX=trainAttrX.reindex(index=valid_index).drop(\"TPO\", axis=1).drop(\"ID\", axis=1)\n",
    "#         validBX=trainImagesX[valid_index]\n",
    "        \n",
    "#         print('==================================================')\n",
    "        \n",
    "#         # create model\n",
    "#         print('Create Model')\n",
    "#         model=model\n",
    "        \n",
    "#         # set an earlystop to avoid overfitting\n",
    "#         earlystop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "#         # fit the model\n",
    "#         fit_stats = model.fit(x=[trainAX, trainBX], y=trainY, batch_size=batch_size, epochs=epochs, \n",
    "#                               verbose='auto', callbacks=earlystop, \n",
    "#                               validation_data=([validAX,validBX],validY), shuffle=True)\n",
    "        \n",
    "#         # print validation accuracy score \n",
    "#         acc=fit_stats.history['accuracy']\n",
    "#         los=fit_stats.history['loss']\n",
    "#         val_acc=fit_stats.history['val_accuracy']\n",
    "#         val_loss=fit_stats.history['val_loss']\n",
    "#         epoch=range(len(acc))\n",
    "#         cvscores.append(np.mean(val_acc))\n",
    "#         print(\"Accuracy: %.2f%%\" % (np.mean(val_acc)*100))\n",
    "        \n",
    "#         # display learning curve\n",
    "#         plt.plot(epoch, los, 'r', label='Training Loss')\n",
    "#         plt.plot(epoch, val_loss, 'b', label='Validation Loss')\n",
    "#         plt.title('Model Loss')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         plt.plot(epoch, acc, 'r', label='Training Accuracy')\n",
    "#         plt.plot(epoch, val_acc, 'b', label='Validation Accuracy')\n",
    "#         plt.title('Model Accuracy')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#     # show the training results    \n",
    "#     accuracy = np.mean(cvscores)\n",
    "#     std=np.std(cvscores)\n",
    "#     print('CV_Accuracy:%.2f%%(+/- %.2f%%)'%(accuracy*100,std*100))\n",
    "#     model.save('best_cv_multi_model.h5')\n",
    "    \n",
    "#     # show evaluate score\n",
    "#     test_score=model.evaluate(x=[testAttrX, testImagesX], y=testY, batch_size=batch_size, verbose=1)\n",
    "#     print(\"%s%s: %.2f%%\\n\" % (\"evaluate \",model.metrics_names[1], test_score[1]*100),\n",
    "#           \"%s%s: %.2f%%\" % (\"evaluate \",model.metrics_names[0], test_score[0]*100))\n",
    "#     return accuracy, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train single input model with cross validation fold\n",
    "def train_model_with_cv(x, x_test, y, y_test, model, cross_validation, epochs, batch_size):\n",
    "    print('Train Model')\n",
    "    print(\"==================================================\")\n",
    "    # create model\n",
    "    print('Create Model')\n",
    "    model=model\n",
    "    print(\"==================================================\")    \n",
    "    # set an earlystop to avoid overfitting\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    print('Cross Validation')\n",
    "    # fit the model\n",
    "    cv=list(range(1, cross_validation+1)) #set number of cross validation\n",
    "    iteration=1\n",
    "    cvscores=[]\n",
    "    cvloss=[]\n",
    "    for i in cv:\n",
    "        print('==================================================')\n",
    "        print(\"Iteration=\", iteration)\n",
    "        iteration+=1\n",
    "        fit_stats = model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, \n",
    "                            verbose='auto', callbacks=earlystop, validation_split=0.2, shuffle=True)\n",
    "        \n",
    "        # print validation accuracy score \n",
    "        acc=fit_stats.history['accuracy']\n",
    "        los=fit_stats.history['loss']\n",
    "        val_acc=fit_stats.history['val_accuracy']\n",
    "        val_loss=fit_stats.history['val_loss']\n",
    "        epoch=range(len(acc))\n",
    "        cvscores.append(np.mean(val_acc))\n",
    "        cvloss.append(np.mean(val_loss))\n",
    "        print(\"Val_Accuracy: %.2f%%\\n\" % (np.mean(val_acc)*100),\n",
    "              \"Val_Loss: %.2f%%\\n\" % (np.mean(val_loss)*100))\n",
    "        # display learning curve\n",
    "        plt.plot(epoch, los, 'r', label='Training Loss')\n",
    "        plt.plot(epoch, val_loss, 'b', label='Validation Loss')\n",
    "        plt.title('Model Loss(iteration={})'.format(i))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.plot(epoch, acc, 'r', label='Training Accuracy')\n",
    "        plt.plot(epoch, val_acc, 'b', label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy(iteration={})'.format(i))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    # show the training results    \n",
    "    accuracy = np.mean(cvscores)\n",
    "    loss = np.mean(cvloss)\n",
    "    std=np.std(cvscores)\n",
    "    loss_std = np.std(cvloss)\n",
    "    print('CV_Accuracy:%.2f%%(+/- %.2f%%)\\n'%(accuracy*100,std*100),\n",
    "         'CV_Loss:%.2f%%(+/- %.2f%%)\\n'%(loss*100,loss_std*100))\n",
    "    model.save('trained_model/best_single_model.h5')\n",
    "    \n",
    "    # show evaluate score\n",
    "    test_score=model.evaluate(x=x_test, y=y_test, batch_size=batch_size, verbose=1)\n",
    "    print(\"%s%s: %.2f%%\\n\" % (\"evaluate \",model.metrics_names[1], test_score[1]*100),\n",
    "          \"%s%s: %.2f%%\\n\" % (\"evaluate \",model.metrics_names[0], test_score[0]*100))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multiple input model with cross validation fold\n",
    "def train_multimodel_with_cv(trainAttrX, trainImages, testAttrX, testImages, trainY, testY, \n",
    "                            model, cross_validation, epochs, batch_size):\n",
    "    print('Train Model')\n",
    "    print(\"==================================================\")\n",
    "    # create model\n",
    "    print('Create Model')\n",
    "    model=model\n",
    "    print(\"==================================================\")    \n",
    "    # set an earlystop to avoid overfitting\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    print('Cross Validation')\n",
    "    # fit the model\n",
    "    cv=list(range(1, cross_validation+1)) #set number of cross validation\n",
    "    iteration=1\n",
    "    cvscores=[]\n",
    "    cvloss = []\n",
    "    for i in cv:\n",
    "        print('==================================================')\n",
    "        print(\"Iteration=\", iteration)\n",
    "        iteration+=1\n",
    "        fit_stats = model.fit(x=[trainAttrX, trainImages], y=trainY, batch_size=batch_size, epochs=epochs, \n",
    "                            verbose='auto', callbacks=earlystop, validation_split=0.2, shuffle=True)\n",
    "        \n",
    "        # print validation accuracy score \n",
    "        acc=fit_stats.history['accuracy']\n",
    "        los=fit_stats.history['loss']\n",
    "        val_acc=fit_stats.history['val_accuracy']\n",
    "        val_loss=fit_stats.history['val_loss']\n",
    "        epoch=range(len(acc))\n",
    "        cvscores.append(np.mean(val_acc))\n",
    "        cvloss.append(np.mean(val_loss))\n",
    "        print(\"Val_Accuracy: %.2f%%\\n\" % (np.mean(val_acc)*100),\n",
    "              \"Val_Loss: %.2f%%\\n\" % (np.mean(val_loss)*100))\n",
    "        \n",
    "        # display learning curve\n",
    "        plt.plot(epoch, los, 'r', label='Training Loss')\n",
    "        plt.plot(epoch, val_loss, 'b', label='Validation Loss')\n",
    "        plt.title('Model Loss(iteration={})'.format(i))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.plot(epoch, acc, 'r', label='Training Accuracy')\n",
    "        plt.plot(epoch, val_acc, 'b', label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy(iteration={})'.format(i))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    # show the training results    \n",
    "    accuracy = np.mean(cvscores)\n",
    "    loss = np.mean(cvloss)\n",
    "    std=np.std(cvscores)\n",
    "    loss_std = np.std(cvloss)\n",
    "    print('CV_Accuracy:%.2f%%(+/- %.2f%%)\\n'%(accuracy*100,std*100),\n",
    "         'CV_Loss:%.2f%%(+/- %.2f%%)\\n'%(loss*100,loss_std*100))\n",
    "    model.save('trained_model/best_multi_model.h5')\n",
    "    \n",
    "    # show evaluate score\n",
    "    test_score=model.evaluate(x=[testAttrX, testImages], y=testY, batch_size=batch_size, verbose=1)\n",
    "    print(\"%s%s: %.2f%%\\n\" % (\"evaluate \",model.metrics_names[1], test_score[1]*100),\n",
    "          \"%s%s: %.2f%%\\n\" % (\"evaluate \",model.metrics_names[0], test_score[0]*100))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_curve(y, prob):\n",
    "    fpr,tpr,threshold = roc_curve(y, prob) # compute true positive and false positive\n",
    "    roc_auc = auc(fpr,tpr) # compute auc score\n",
    "    \n",
    "    plt.figure()\n",
    "    lw=2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC Curve (area=%0.3f)' % roc_auc)\n",
    "    plt.plot([0,1], [0,1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
